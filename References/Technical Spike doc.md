# Executive Summary

We evaluated technologies for a “Fireflies-style” meeting recorder that runs entirely from a single landing page. The goal is fast implementation with minimal moving parts. **Key recommendations:** Use the browser’s native MediaRecorder API to capture audio (widely supported since 2021), and upload the recording after the meeting (post-hoc transcription preferred). For speech-to-text, a cloud API like OpenAI Whisper offers excellent English accuracy (~95%+) at low cost ($0.006/min). To send transcripts without user login, integrate a transactional email API (e.g. Brevo or Mailgun) that allows ~100–300 free emails/day. Store text transcripts in a simple cloud database (Firestore’s free tier allows 20k writes/day) to enable later retrieval or analytics. A lightweight front-end (vanilla JS or a minimal framework) can provide a one-page interface: “Record” button, live (or placeholder) discussion tips, transcript display, and email form – all on one screen. A monolithic backend (e.g. Node or Python) should handle file upload, call the STT API, save results, and trigger email sending; this straightforward design favors rapid development and easy debugging. We favor proven, high-level services over building custom ML or infrastructure from scratch. This stack trades some advanced features (e.g. real-time transcription, multi-language support, complex analytics) for simplicity and robustness. Below we detail top options per focus area, then conclude with an end-to-end stack proposal including a flow diagram.

# Research Findings

## 1. Audio Recording (Browser)

**Option 1 – HTML5 MediaRecorder API (Rank #1):** Leverage the built-in MediaRecorder to capture audio from the user’s microphone. This API is broadly supported on modern browsers (Chrome, Firefox, Edge, Safari 14.1+) and records to popular formats (e.g. WebM/Opus by default). *Why fit:* It’s native and requires no plugins, yielding good quality and small file sizes. *Risks:* Older Safari (<14) lack support, and iOS had issues pre-2021, but those are largely moot now. Long recordings should be handled in chunks to avoid huge in-memory blobs or odd browser bugs (e.g. Safari may emit a very large chunk if recording is paused). *Integration Steps:* (1) Use `navigator.mediaDevices.getUserMedia({ audio: true })` to get a stream. (2) Initialize `MediaRecorder(stream)` and call `.start()` with a `timeslice` (e.g. a few seconds) to get dataavailable events periodically. (3) Accumulate audio blobs (or upload chunks on the fly to reduce memory load). (4) On stop, assemble or finalize the Blob and upload it to the server via fetch/XHR. (5) Ensure MIME type compatibility with the STT service (e.g. Opus WebM is accepted by many APIs, or convert to WAV server-side if needed). *Cost:* Free – it runs in-browser. Development time is minimal due to high-level API (few dozen lines of JS for basic recording).

**Option 2 – JS Audio Library or Polyfill (Rank #2):** Use a library like **RecordRTC** or **Recorder.js** to handle recording and fallback logic. These libraries can implement recording via Web Audio API for browsers that lack MediaRecorder (e.g. older Safari) by capturing raw PCM and encoding WAV in JavaScript. *Why fit:* They abstract cross-browser quirks and can provide consistent WAV output. *Risks:* More code to include; client-side encoding of a 30‑min audio to WAV could be CPU-heavy and result in large files (uncompressed ~ 5–10MB/min). Given modern browser support is now solid, this may be overkill unless legacy support is critical. *Integration Steps:* (1) Include the library script. (2) Use its API to start/stop recording. (3) Obtain the recorded blob (often WAV) and send to server. (4) If needed, transcode to a compressed format server-side to reduce upload size. *Cost:* Libraries are open-source; no direct cost. Slight performance cost on client if encoding in JS.

*Decision:* **MediaRecorder is the preferred approach** for simplicity and broad support. We’ll include a lightweight polyfill or notice for the rare unsupported cases. The recording flow stays on the landing page – user clicks “Record” (granting mic permission), the app shows a live timer or waveform, and audio data is captured for later upload. This satisfies the one-page requirement with no redirects.

## 2. Speech-to-Text (Transcription)

**Option 1 – OpenAI Whisper API (Cloud) (Rank #1):** Use OpenAI’s Whisper API for transcription. It’s a hosted version of the Whisper model, known for very high accuracy in English (often >95% for clear audio) and robust to accents. *Why fit:* It’s fast to integrate (simple HTTP API), and **cost-effective**: ~$0.006 per minute of audio, so a 30-min meeting costs ~$0.18. That is 3–4x cheaper than Google or AWS STT on a per-minute basis. It supports only batch mode (no real-time stream, which is fine for post-hoc). *Risks:* Requires sending audio to a third-party (privacy considerations). Also, being an API, there’s dependency on uptime and the need to secure the API key on the backend. *Integration Steps:* (1) After recording, POST the audio file to OpenAI’s transcript endpoint (with the API key in header). (2) Receive JSON with the transcribed text. (3) Optionally, use model parameters (e.g. specify language as English to improve speed). (4) Handle errors (e.g. file too large – Whisper supports ~25MB by default, so consider splitting audio if needed). *Rough Cost:* **$0.36/hour of audio** (i.e. $0.18 per 30 min). No monthly fees, pay-per-use.

**Option 2 – Big Cloud STT (Google/AWS/Azure) (Rank #2):** Use a well-known speech API like **Google Cloud Speech-to-Text** or **AWS Transcribe**. *Why fit:* These are very reliable and support features like speaker diarization and word timestamps. Google STT supports 120+ languages (not crucial for English-only) and has proven accuracy for meetings (especially with phone or video models). AWS integrates with S3 and offers vocabulary customization. *Risks:* They have slightly higher costs and more complex setup. Google’s standard model is about $0.016/min (after 60 min free per month) – roughly $1 per hour – and requires a Google Cloud project setup. AWS Transcribe is ~$1.44/hour ($0.0004/s), with a 60-minute free tier per month for a year. These services sometimes run asynchronously (e.g. AWS requires starting a job and polling results), adding complexity. *Integration Steps (Google example):* (1) Enable the Speech-to-Text API in Google Cloud, secure credentials. (2) Upload the audio file to cloud storage (Google’s API can accept a direct file upload for <60s; longer audio may need Cloud Storage URL). (3) Call the API (synchronous for ~30min might work with `longrunningrecognize` or V2 batch). (4) Poll for transcription result, then retrieve text. (5) Parse and format the transcript. *Rough Cost:* **~$1 per hour** (Google standard model); AWS ~~$1.44/h. Both offer some free minutes monthly. Azure’s pricing is similar (~~$1–2/hr depending on model).

**Option 3 – Self-Hosted Engine (Whisper or Vosk) (Rank #3):** Deploy an open-source ASR model on our server (e.g. Whisper small/medium or Vosk). *Why fit:* Avoids sending data externally – good for privacy or offline use. Once set up, transcribing has no incremental cost aside from compute. *Risks:* **Heavy infrastructure** for 30-min audio: Whisper-large model requires a GPU to run in near real-time; on CPU it could take 2–3× audio length or more. Real-time “live” transcription is impractical without significant compute. Maintenance and scaling are non-trivial (would need to run a worker service). *Integration Steps:* (1) Set up a server (with GPU if using larger model) or use an API like HuggingFace Inference or AssemblyAI’s local deployment. (2) When audio is received, feed it to the model (could use Python libraries like OpenAI’s `whisper` or Vosk’s API). (3) Wait for transcription (could be several minutes for 30-min audio on CPU). (4) Return text. *Rough Cost:* If using cloud GPU, could be ~$0.5-$1 per hour of use. Running on CPU is “free” but slow. Maintenance cost is high in engineering time. Given the priority on speed and simplicity, this is the least preferred unless data privacy rules mandate it.

*Decision:* **OpenAI Whisper API is the top choice** for English transcription due to its combination of accuracy and ease. For MVP, we trade the advanced options (speaker labels, etc.) for a quick transcript. A 30-min meeting can be transcribed in roughly real-time or a few minutes delay, which is acceptable post-call. We’ll design the backend to accept an audio file, call the Whisper API, and get the text. If needed in future, the abstraction allows swapping in Google or an on-prem model. The one-page app will simply display the transcript once ready (e.g. show a “Transcribing…” spinner for a short period after upload).

## 3. Email Delivery (No-Auth User)

**Option 1 – Transactional Email API (SendGrid/Mailgun/Brevo) (Rank #1):** Use a third-party email service via API to send the transcript to the user’s email. *Why fit:* These services are built for transaction emails (alerts, one-off sends) with high deliverability. **SendGrid** (by Twilio) was popular and Fireflies itself uses SendGrid, but as of mid-2025 its free tier is discontinued. Alternatives like **Brevo (Sendinblue)** offer ~300 emails/day free, or **MailerSend** with pay-per-use ($1 per 1k emails). *Risks:* Requires API key management and possibly domain setup for best results (though most offer reasonable deliverability via their domains/IPs initially). Also need to guard against abuse (someone could use our form to spam others – we should at least do a simple CAPTCHA or limit). *Integration Steps:* (1) Sign up for a service and get an API key. (2) Use their REST API or SDK to send an email with the transcript text (either in body or attachment). E.g., Mailgun provides a simple HTTP POST endpoint with form data (from, to, subject, text). (3) Configure sender identity: use a verified domain or a default sender (e.g. “[no-reply@ourapp.com](mailto:no-reply@ourapp.com)”). (4) Handle API response for errors (e.g. bounced email address format). (5) Optionally, log email status (most APIs can ping a webhook on delivery events, but for MVP we might skip). *Rough Cost:* **Free for low volume.** For instance, Brevo’s free plan (300 emails/day) covers our needs initially. Paid tiers start ~$9–20/month for higher volume. If usage grows, cost per email remains low (Mailgun ~$0.80 per 1000 after free tier).

**Option 2 – AWS Simple Email Service (SES) (Rank #2):** Use AWS SES to send emails through our backend. *Why fit:* Extremely cheap at scale ($0.10 per 1,000 emails sent) and high deliverability if configured properly. *Risks:* Setup is non-trivial – it requires verifying a sender domain and staying in SES “sandbox” limits initially. Also, SES by itself is just an SMTP/API – we’d have to implement our own retry logic and handling of bounces unless using Amazon’s notification service. *Integration Steps:* (1) Configure an AWS SES domain or email identity and get it out of sandbox (which requires some request to AWS if we want to send to arbitrary addresses). (2) Use AWS SDK in code to send an email (or use SMTP interface via something like Nodemailer). (3) Handle errors for rejected sends. Ensure DKIM/SPF are set on the domain for deliverability. *Rough Cost:* Virtually **$0** at our scale (first 3,000 emails/month free on EC2, afterward $0.10/1k emails). But engineering overhead is the main cost.

**Option 3 – SMTP via Existing Email (Rank #3):** As a quick fix, use a free SMTP server (e.g. Gmail’s SMTP or a hosting provider’s) to send mail. *Why fit:* No new services to sign up if we already have an email. *Risks:* Gmail SMTP has send limits (~500/day) and may get flagged as spam if used outside its intended purpose. Sending directly from our server’s IP is also risky (likely to be marked spam without proper DNS records). This approach is not very scalable or robust. *Integration Steps:* (1) Configure an SMTP client (Nodemailer or similar) with the credentials (for Gmail, an App Password is needed). (2) Compose the email and send via SMTP. (3) Monitor for errors. *Rough Cost:* Free (just uses an email account), but not suitable beyond testing.

*Decision:* **Use a transactional email API (Brevo/Mailgun)** for ease and reliability. For MVP, we can start with a free-tier account – e.g. Brevo allows 9,000 emails/month free which is plenty for a new app. This requires minimal code: just an HTTP POST in our backend after transcription is done. We’ll send the transcript as plain text content (since it’s text-only) and perhaps also attach it as a .txt file. No user authentication is required on our side; we simply take the email address input from the page (with validation) and use the API. We should implement basic rate limiting or at least email validation to prevent misuse. Security-wise, using the API key on the server side keeps it hidden from the user. This approach keeps the entire flow on our single page: user enters their email and clicks “Send Transcript” – our backend handles the rest and we can show a success message on the page.

## 4. Storage of Transcripts

**Option 1 – Managed NoSQL DB (Firebase Firestore or similar) (Rank #1):** Use a cloud-hosted JSON document database to store transcripts and minimal metadata. *Why fit:* Firestore (Firebase) is schemaless and very easy to use, with generous free quotas – 1 GiB storage, 20k writes/day free, which covers an MVP. It auto-scales, so if the app grows, we don’t worry about provisioning. We can store each transcript as a document with fields: ID, email, timestamp, transcript text. If we ever add a frontend to list past transcripts (maybe via email link), this DB can serve those. *Risks:* Data is hosted by Google – consider privacy (transcripts might contain sensitive info). Firestore is eventually consistent; but for our use (reading right after writing if needed), it’s fine. GDPR compliance would require informing users that their data is stored in cloud and possibly allowing deletion. *Integration Steps:* (1) Set up Firebase project, enable Firestore. (2) Use Firebase Admin SDK on the backend to add a document when a transcript is ready. (3) (Optional) Use client SDK if we want to retrieve for UI, but since no auth, probably backend only. (4) Secure the database (rules or use only via server, which is safest). *Rough Cost:* **Free for thousands of transcripts** – e.g. 50k reads/day and 20k writes/day at no cost. Beyond that, pay-as-you-go (reads ~$0.06 per 100k, writes $0.18 per 100k, which is very low).

**Option 2 – Relational DB (SQLite/Postgres) (Rank #2):** Use a traditional SQL database to store transcripts. *Why fit:* Familiar and **robust**. A single-file SQLite could work for a small app – extremely quick to set up (just write to a .db file). If we deploy on a service, a managed Postgres (e.g. Supabase, which has a free tier) could be used for multi-instance scaling. *Risks:* SQLite in a serverless or multi-instance environment is problematic (concurrency and no persistence across restarts unless using a mounted volume). Postgres requires upfront setup and an external host if using serverless backend. For scaling, we’d need to manage migrations, etc. Also, transcripts might be large text (though usually a few pages of text – easily handled by SQL). *Integration Steps:* (1) If using SQLite: initialize a database file and a table with columns for id, email, text, etc. Write to it when a transcript is ready. Ensure file is persisted on disk. (2) If using Postgres: set up DB (cloud or container), run migrations for a transcripts table. Use a server ORM or query builder to insert/retrieve data. (3) Implement deletion or cleanup policies if needed (GDPR: maybe auto-delete after X days or on request). *Rough Cost:* SQLite – free. Postgres – can use free tier on e.g. Heroku or Supabase (Supabase free can handle a small app easily). Paid Postgres small instance ~$15-20/month if scaling up.

**Option 3 – Flat File Storage (Cloud Object or Local) (Rank #3):** Store each transcript as a text file, named by an ID, in a storage bucket or the server filesystem. *Why fit:* Very simple: no database code at all, just one file per meeting. Could use AWS S3 or similar – which is durable and cheap ($0.023/GB). *Risks:* Without a database index, querying or listing transcripts is hard (we’d need to track file names somewhere). If just emailing out and mostly not retrieving via UI, we might not need complex querying – but if a user repeats a session, we might not easily connect them to their past transcripts unless we include links in email. Also, handling concurrent writes or updates is easier with DB. Local disk storage is ephemeral if the service restarts or scales, so cloud bucket is safer. Privacy: storing in raw text files is fine but ensure proper access controls on the bucket (private). *Integration Steps:* (1) After transcription, save the text to a file (e.g. generate a UUID for filename). (2) Use an object storage SDK/API to upload that file (if using cloud storage). (3) Save the file URL or name if needed for reference (could email a link, but better to just email the content directly). (4) Set lifecycle rules on the bucket if we want to auto-delete files after some time. *Rough Cost:* Minimal – e.g. 10,000 transcripts of 5KB each is ~50 MB, costing <$0.01 on S3. Bandwidth for occasional download negligible.

*Decision:* **Firestore (NoSQL) is our pick** for its zero-ops convenience and free scaling. We’ll store each transcript document keyed by an auto-ID or meeting ID. This allows future extension (e.g. if we want a simple “email me a copy later” link or to avoid duplicate emailing). In the MVP, the main use of storage is to have a persistent log (so we don’t lose transcripts if an email fails to send) and possibly to support a “Discussion Questions” feature. If we want to implement **live “discussion questions” during recording**, one approach is to generate some prompts ahead or use partial transcripts: We could periodically send chunks to the STT (or use a lightweight on-device model) to get interim text, then run an AI model to suggest a question. This is complex for MVP; instead, we might show a static list of generic discussion questions on the page or simply leave a section for notes. Since transcripts are persisted, we could also analyze past meetings to suggest questions next time (out of scope for now). Regarding privacy/GDPR: we will inform users that transcripts are stored and emailed, and allow them to request deletion (which is easy by deleting the Firestore doc). All stored text should be encrypted at rest by the cloud provider (Firestore does this by default).

## 5. Frontend (One-Page App UI/UX)

**Option 1 – Minimal Vanilla JS or Micro-Framework (Rank #1):** Build the interface with plain HTML/CSS and a sprinkle of JS (or use a tiny reactive lib like **Alpine.js** or **Stimulus** for convenience). *Why fit:* The app scope is small – a single page with a record button, a text area (transcript), maybe a live status, and an email input. This can be done without a heavy framework, making the app load fast and avoiding build tooling (just open in browser). Developer can manage state (recording vs processing vs done) with simple JS events. Libraries like Alpine (under 10kB) allow binding UI state to elements declaratively, which is nice for showing/hiding the “discussion questions” panel or “Send” button. *Risks:* If the team is more comfortable with React/Vue, vanilla could slow them down or result in less structured code. But given emphasis on lightweight and “fast implementation”, avoiding a large framework reduces complexity. *Integration Steps:* (1) Design an HTML layout: sections for recording controls, live status (“Recording… elapsed time”), a placeholder for live or final transcript, a sidebar or box for “Discussion Questions”, and a form for email submission. Everything is on the same page to meet the requirement. (2) Use CSS (or a simple framework like Tailwind or Bulma) for styling buttons and layout. (3) Write JS to handle button clicks: start/stop recording (hook into MediaRecorder), update the UI state (e.g. change the record button to a stop button and show elapsed time via `setInterval`). (4) After stop, show a “Transcribing…” indicator, then send the audio blob to backend (AJAX call). (5) When the transcript is returned (or fetched via polling/WS), display it in a scrollable . (6) Show the email input and send button, handle its click by calling backend to email, then confirm to user. *Rough Cost:* Free. Only developer time. Performance wise, loading a simple page is quick; no heavy JS bundles.

**Option 2 – Modern JS Framework (React/Svelte/Vue) (Rank #2):** Use a popular framework to build a more structured single-page app. *Why fit:* If the team has experience in one of these, it might speed development despite the overhead. **React** is widely known; using it with a UI library (e.g. MUI or Chakra) could quickly yield a polished UI. **Svelte** is another great choice here – it compiles to very small bundle (~1.6KB runtime vs React’s ~42KB), meaning our app stays lightweight but with reactivity built-in. Svelte’s state management might simplify showing live transcriptions or reactive UI updates. *Risks:* Any framework adds a build step and more dependencies. React in particular would be overkill for just one or two components, and would increase load size and complexity (but it’s proven and the team might prefer it). Svelte is less common in some teams, but known for fast performance and straightforward state handling (e.g. bind to a variable for transcript text). Vue or others similarly could work. *Integration Steps:* (1) Scaffold an app (e.g. create-react-app or Vite for Svelte). (2) Build a component for the recording control, which manages MediaRecorder and state. (3) Build a component for the transcript display and questions. (4) Build a component/form for email sending. Use context or props to pass the transcript data around or centralize in state. (5) Use fetch or Axios to call backend endpoints. (6) Bundle and deploy the static files. *Rough Cost:* Development might be slightly longer if starting from scratch, but if the team already has a preferred stack, it could be fine. Performance impact is minor for our use (even 50KB of JS is fine on modern connections, and the app is internal use likely). No licensing costs.

**Option 3 – Mobile-Friendly or Desktop App Wrapper (Rank #3):** (Though the focus is web, for completeness) one could implement the UI in a cross-platform toolkit (Electron, Tauri, etc.) or ensure mobile web compatibility. *Why fit:* If users might access from phone to record in-person meetings, a responsive web UI is needed. This is more of a design consideration than a separate tech stack. We assume web page usage on desktop primarily. *Risks:* Developing a separate mobile app or Electron app is out of scope for now, and not required.

*Decision:* **We’ll implement a straightforward single-page web UI with minimal JS**. The goal is that from the landing page, the user can do everything: hit “Record” (the page dynamically shows recording status), see either live transcription snippets or at least have a “questions” panel (which might for now display static tips or remain empty), then stop and see the final transcript appear, and enter their email to send it – all without navigating away. We will ensure the page is usable on modern browsers (desktop and mobile Safari/Chrome). Using plain JS or a tiny framework avoids a heavy build process, aligning with a quick “terminal + Claude Code” workflow (likely meaning we or an AI pair-programmer can quickly iterate on it). Svelte is an attractive middle-ground if we want reactive features (for example, updating a live word-by-word transcript if we later support it), but initially, the added build step might not be justified. We favor **lowest complexity**, so likely no build system: just serve an `index.html` with an inline script or small bundle. This also simplifies deployment (could even be a static page served by the backend).

## 6. Architecture & Deployment

**Option 1 – Monolithic Web App (Rank #1):** Build a single backend service that handles all functions: receiving audio, calling STT, storing transcript, and sending email. *Why fit:* **Simplicity and speed.** A monolith means fewer deployment units and easier debugging – all logic in one place. This suits a small team and early-stage project. We can use a framework like **Express (Node.js)** or **Flask/FastAPI (Python)** to create a few endpoints: e.g. `POST /uploadAudio` and `POST /sendEmail`. The service can be stateful (keeping track of the transcription status in memory or DB) or stateless if each request contains all info needed. In practice, we might do the processing in one request: the client uploads audio, the server processes it and responds with the text (or perhaps we use a simple polling if it takes long). *Risks:* If the transcription is time-consuming (say >30s), the HTTP request might time out. In that case we’d need to offload to a background thread or use an async response. But since we expect ~5 min max for STT, we can increase timeouts or handle asynchronously (e.g. respond immediately with a job ID and poll it). Monolith might be less scalable if each audio processing ties up the server – but initially volume is low. We should implement error handling: e.g. if STT API fails or email API fails, the server should return a clear error so the UI can inform the user or retry. *Integration Steps:* (1) Develop the server with routes for each needed action. (2) Within the audio upload route: accept file (possibly using multipart form or base64 JSON upload), save to temp storage, call STT (await result). (3) On success, save transcript to DB and return the text to client. Also, possibly trigger email send here or on a separate request. (We could also have the client call a separate `/emailTranscript` endpoint with the transcript ID and email, to decouple transcription from emailing). (4) Deploy this server on a platform (could be a simple Node server on Heroku/Fly.io or a container). (5) Implement basic logging and error recovery: e.g. if email fails, maybe try once more or return error so user can re-attempt. *Performance tweaks:* Use streaming or chunked upload to handle large files, or require the client to compress audio (we already plan to record in Opus which is ~X times smaller than WAV). For 30 min (~5-10 MB Opus), an upload over broadband is a few seconds – acceptable. On the server, calling external APIs is the slowest step; we ensure those calls are async and the server can handle multiple in parallel (use async IO or threading). For memory, we might not want to load a whole 50MB WAV into RAM – but using Opus ~10MB, that’s fine. We will clean up temp files promptly.

**Option 2 – Serverless / Microservice Pipeline (Rank #2):** Split functionality into separate services or functions. For example: an **upload function** that writes the audio to cloud storage and enqueues a transcription job, a **transcription worker** that picks it up and calls STT, then a **mailer service** to send the email. *Why fit:* This can scale better and isolates failures (e.g. if transcription crashes, it doesn’t take down the webserver). Using serverless functions could mean no managing servers at all. E.g., upload to S3 triggers an AWS Lambda to process and save result, then call SES to email. *Risks:* More moving parts = more development time and points of failure for an MVP. Orchestration (like using AWS SQS or Step Functions) is needed to connect functions. Debugging asynchronous flows is harder. Cold starts might add latency. Also, for a synchronous user flow (user waiting for transcript), a fully async approach complicates giving feedback. *Integration Steps:* (1) Set up a storage trigger or message queue for audio files. (2) Lambda Function A: On audio upload, call STT API. (Could also do this via an API Gateway call synchronously – but then similar to monolith). (3) Lambda Function B: On receiving text (perhaps put into a DB), send email. Or function A directly calls email API after transcription. (4) Manage state via a DB or passing context through payloads (like include the user’s email address in the job message). *Rough Cost:* Could be cheap at low usage (Lambdas on AWS free for first million requests and charged per ms after; our usage is small). But the complexity likely outweighs benefits for now.

**Option 3 – P2P or Browser-Only (Experimental) (Rank #3):** Try to do everything on the client side – e.g. use a WebAssembly version of Whisper in the browser, and use a client-side SMTP API. *Why fit:* This would avoid any backend at all (truly no-auth, no server needed beyond hosting the static page). *Risks:* Not very feasible in 2025: Whisper models are too heavy to run in-browser for 30 min audio (would grind the browser or take extremely long). And sending email from the browser requires an email-sending API anyway (unless using mailto: which is not automatic). So this is more a thought experiment.

*Decision:* **A monolithic server architecture is recommended** for speed of development and straightforward deployment. All essential logic will reside in one codebase – easier to “hack in” new features during rapid prototyping. For example, if we later add an AI summary, we can call that in sequence after transcription, all within the same service. We’ll ensure the architecture can handle errors gracefully: e.g., if the STT API fails, respond to client with an error message (“Transcription failed, please try again”). If email fails, perhaps return that status so the user can copy the transcript manually. Performance considerations: a single process can handle multiple requests; if usage grows, we might add basic concurrency (Node’s event loop can overlap I/O, or in Python use Celery/RQ for the heavy transcription tasks). Given a 30-min audio might take, say, 1-3 minutes to process on the API side, we might handle a few concurrently on one server instance. This should suffice for MVP volume. We’ll also include simple **logging and monitoring** to catch issues (e.g. how long STT calls take, how often emails fail).

In terms of deployment, we can containerize this app and run it on a platform like Heroku or Fly.io for ease. Alternatively, use a managed service (e.g. Vercel for the static front + a serverless function for backend logic, but processing 30-min audio might exceed typical function time limits). A small VM or container is likely better due to the long processing. For now, one instance is fine – no microservices needed. This architecture optimizes developer velocity, aligning with advice that early-stage startups often benefit from monoliths for quick iteration. We can always refactor later if needed.

## 7. Competitive Scan (Meeting Transcription Tools)

**Fireflies.ai – AI Notetaker:** Fireflies is a mature product that inspired this feature set. It automatically joins online meetings (via a bot or integration) and records & transcribes them. It boasts ~90% accuracy transcripts and multi-language support (70+ languages). It also provides AI-generated summaries, action items (“Ask Fred” assistant), and integrations with apps like CRM, Slack, etc.. *How they do it:* Fireflies uses a cloud backend (their stack includes Node.js and Python, with ML frameworks like TensorFlow). They store data in both MySQL and MongoDB, and use S3 for audio storage. For transcription, they likely leverage big cloud APIs or custom models; they advertise speaker identification and have ~90% accuracy even for accents. Fireflies emphasizes post-meeting processing – transcripts and summaries are ready a few minutes after the call. They have a Chrome extension (used to allow recording without a visible bot) but it was retired, possibly due to platform policies. *Key lessons:* Fireflies’ strength is in **workflow automation**: it emails notes to attendees, allows searching across past meetings, and ensures security (SOC 2, GDPR compliance). For our simpler app, we note that Fireflies uses SendGrid for emails and a monolithic+modular architecture (Meteor/Next.js for front-end, AWS for hosting). They show that using proven services (cloud storage, email API, etc.) scales to millions of users. However, Fireflies’ rich features (live collaboration on notes, analytics on talk time) come with complexity we don’t need initially.

**Otter.ai – Live Transcription & Notes:** Otter focuses on real-time transcription with a user-friendly interface. It can join meetings via “OtterPilot” and display live captions, and generates summaries after. It’s known for being mobile-friendly and has a generous free tier (300 min/month). Accuracy is decent (85–90%) but not perfect, and it sometimes mis-identifies speakers. Unlike Fireflies, Otter is more about immediate transcription and collaboration – users can highlight text during the meeting and add comments. *Takeaways:* Otter proves real-time STT is valuable for user engagement, but our user indicated real-time isn’t necessary for MVP. Implementing live transcription would require streaming audio to an API that supports it (Google/AWS can stream, but with more integration effort and cost). Since we prefer post-hoc (cheaper and simpler), we accept that our app will behave more like Fireflies (after-meeting transcripts) than Otter’s live notes. Otter also has no email requirement for basic use (transcripts viewed in app), but in our flow emailing is a core feature. This is fine given our one-page design.

**Newer Entrants – Krisp AI & Others:** **Krisp.ai**, initially a noise-cancellation tool, has added meeting transcription with an emphasis on audio quality. Reports show Krisp’s accuracy can reach ~92% due to superior noise handling – higher than Otter’s under noisy conditions. It lacks some of the collaboration features, focusing on producing a clean transcript. *Insight:* High audio quality input yields better transcripts for any engine. This suggests we encourage users to use good mics or noise-free environments. In design, maybe display a tip like “Ensure you’re in a quiet place for best results.” Krisp’s example also hints that for differentiation, we could incorporate some noise suppression in recording (the Web Audio API could do basic noise filtering or we could apply a filter before sending to STT, though likely not needed if STT is robust).

**Open-Source Alternatives:** There’s movement toward self-hosted meeting assistants. For example, **Meetily** is an open-source app that records and transcribes meetings locally, using Whisper and even local LLMs for summaries. It emphasizes privacy (all data on user’s infrastructure). Another, **Amurex** (Chrome extension + server) was built by a developer to fix issues he saw: it records meetings quietly (no bot), delivers transcripts right after, drafts follow-up emails, stores past meetings, and even gives *real-time suggestions to stay engaged* during the meeting. This real-time suggestion feature is exactly akin to “live discussion questions.” *How they achieve that:* Likely the extension streams audio to a local or cloud STT in real-time and uses an LLM to generate questions based on content. This is complex but doable with a pipeline of partial transcription → prompt an AI model. For our product, implementing this in the future might involve using a streaming STT API (like Deepgram or AssemblyAI which allow real-time) and a lightweight model for Q&A. But given the added complexity and our decision for post-hoc transcription, we will probably omit true “live” AI questions in v1. Possibly we can simulate it by showing a static list of generic thoughtful questions, or by updating a question at midpoint using whatever audio has been transcribed so far (if we implement a halfway transcription). The open-source entrants teach us that focusing on a few key features (transcription accuracy, immediate availability, and summarization) can carve a niche. They also show feasibility of drafting follow-up emails automatically – something Fireflies and Otter now do too. For now, we simply provide the raw transcript and let the user derive notes, but we could integrate an AI summary in future (e.g. send transcript to GPT-4 to get a summary and questions).

In summary, competitors are converging on similar features: multi-platform recording, accurate transcripts, AI summaries, and integrations. Our MVP will nail the basics with a lean stack: **fast setup, minimal user friction (no login, one-click record), and reliable email output**. This addresses the core user need (having a transcript in their inbox) without the overhead of full meeting analytics. By choosing stable, high-level solutions in each area (browser APIs, cloud STT, trusted email APIs, managed storage), we emulate the successful patterns of Fireflies/Otter on a smaller scale, prioritizing robustness and speed of delivery over breadth of features.

# Stack Recommendation

Taking the top options, here’s the proposed end-to-end solution and flow:

**Frontend/UI:** A single-page web app (HTML/JS) served at the landing URL. It presents a **“Record” button**, which when clicked will request mic access and start recording via the **MediaRecorder API**. While recording, the page can show a timer and perhaps a static “Discussion Questions” box (with preset questions or left blank for now). When the user hits **“Stop”**, the app stops recording and gets the audio Blob. It immediately calls the backend to upload this file and then shows a “Transcribing…” indicator. Once a response comes back with the transcript text, the page displays the **transcript** in a scrollable area. The user can then enter their email in an input field and press **“Send Transcript”**. This triggers an AJAX call to the backend to send the email, and on success we show a confirmation (“Transcript sent to ___”). All these steps happen on one page without reloads.

**Backend:** A monolithic server (let’s say built with Node.js Express for concreteness) with two main endpoints:

1. **POST `/transcribe`** – accepts the audio blob (multipart form or binary). It saves the file (or reads it into memory), then calls **OpenAI Whisper API**. Upon receiving the transcription, it stores the text in **Firestore** (for persistence, using an auto-ID or hash of content). It returns the text (and maybe an ID).
2. **POST `/email`** – expects an email address (and perhaps a transcript ID or the text itself). It uses the **Brevo/Mailgun API** to send an email from our service (e.g. from “[notes@ourapp.com](mailto:notes@ourapp.com)”) to the user with the transcript. It returns success/failure.

These can be combined into one endpoint for simplicity (transcribe then email), but separating allows the user to review the text before emailing. Also, if transcription takes time, we might not want the HTTP response to hang; but since our UI will show a loading state, we can afford to keep the request open for, say, up to 2 minutes. Alternatively, implement a polling: `/transcribe` immediately responds with a job ID, and the client polls `/status?id=123` until ready. However, given moderate audio length and using fast cloud STT, a direct wait is acceptable for MVP.

**Flow Diagram:** (client on left, server on right)

```
[Browser]                              [Node Backend]
    | (1) getUserMedia()
    |     & MediaRecorder start
    |-----------------------------------> (User grants mic, no server interaction yet)
    |
    | (2) on stop: POST /transcribe (audio blob) -----> [Receive audio file]
    |                                                  [Call Whisper API -> get text]
    | <----------------------------- JSON response (transcript text)
    |
    | (3) Display transcript to user
    | (User enters email and hits Send)
    | (4) POST /email {email, transcript_id} -------> [Lookup transcript from DB]
    |                                                [Send via Email API e.g. Brevo]
    | <----------------------------- JSON response (email sent OK)
    |
    | (5) Show confirmation UI

```

*(Step (2) could alternatively respond immediately with a job ID, and the server later POSTs the result via WebSocket or client polls. But for clarity, we shown synchronous flow.)*

**Stack Summary:**

- **Client:** HTML5, possibly Alpine.js for reactive UI, using MediaRecorder and fetch APIs – no build needed.
- **Server:** Node.js with Express (one process). Could also be Python FastAPI – both are fine; Node might handle concurrent I/O more easily.
- **Speech-to-Text:** OpenAI Whisper API (English only, batch).
- **Email:** Brevo (Sendinblue) API for SMTP (free 300/day, upgrade as needed).
- **Database:** Firebase Firestore for transcripts (1 GiB free, auto scale).
- **Hosting:** The server can be containerized and run on a service (Heroku/Fly). The static frontend can be served by the same Express or from Firebase Hosting (10GB free) if separated. For simplicity, Express can serve the static files and handle API routes (monolith approach).

This stack is geared towards **quick development and reliability**. Each component is a well-documented service or API, reducing custom code. We prioritize error handling: e.g., if STT fails, respond with an error and let the user retry; if email fails, we can allow another attempt or display an error (the transcript is still on screen as backup). Performance should be sufficient: cloud STT and email APIs handle the heavy lifting, and our server just orchestrates. To scale up, we might enable parallelism (multiple Node workers or deploy multiple instances behind a load balancer), and all state (DB and external APIs) is external so the app is stateless.

In conclusion, we recommend **implementing the one-page recorder app with this stack**. It delivers the core functionality (record -> transcribe -> email) using proven solutions, and positions us to later integrate enhancements (like AI summaries or live questions) without having to rework the fundamentals. By focusing on robustness and simplicity now, we ensure a stable base that can be extended once we gather user feedback on what features matter most (e.g. if live discussion prompts are a hit, we can invest in real-time STT streaming). This approach mitigates risk and meets the immediate goal: *a Fireflies-like experience achievable with minimal fuss.*

# References

- Mozilla Developer Network (2025). *MediaRecorder API – Recording Media Streams*
- LambdaTest (2025). *Browser Compatibility of MediaRecorder on Safari*
- Addpipe Tech Blog (Sep 2024). *Dealing With Huge MediaRecorder Chunks*
- Holori AI Pricing Guide (2025). *OpenAI Whisper transcription cost*
- NearHub Blog (May 2025). *AWS Transcribe Pricing Explained – $0.0004/s (=$1.44/h)*
- Google Cloud Docs (2023). *Speech-to-Text Pricing – ~$0.016/min after free 60 min*
- Fireflies.ai Tech Stack – Himalayas (2025). *Fireflies uses Node, Python, SendGrid, S3, MySQL/Mongo*
- SuperAGI Blog (June 2025). *Otter vs Fireflies vs Krisp – Accuracy & features*
- Bardeen AI Guide (Last updated 2025). *Fireflies supports 70+ languages, GPT-3 summaries*
- Stackfix Review (2025). *Fireflies vs Read – Fireflies transcriptions high quality, no bot recording*
- Reddit (Feb 2025). *Amurex open-source tool – records quietly, follow-up emails, real-time suggestions*
- Kahunam Tech Blog (Jul 2025). *SendGrid free plan ending; Brevo 300/day free, SES $0.10/1000 emails*
- Dev.to (Sep 2024). *Firebase Free Tier – Firestore 1GB storage, 20k writes/day free*
- DZone (Jan 2024). *Monolith vs Microservices for Startups – monolith simplifies rapid prototyping*